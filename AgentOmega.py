#!/usr/bin/env python3
"""
OCTA AgentΩ — Orthospace Attractor + Multi-Agent + Perfect-T Crypt
Bootstrap script.

Run this ONCE:

    python agentomega_bootstrap_v2.py

It will create a fully-wired repo:

    oxta_agentomega/
        pyproject.toml
        README.md
        .gitignore
        quickstart.sh
        agentomega_src/
            agentomega/
                __init__.py
                llm.py
                tools.py
                tasks.py
                orthospace.py
                attractor.py
                rewards.py
                perfect_t_crypt.py
                curriculum.py
                executor.py
                multi_agent.py
                run_loop.py
                metrics.py
                benchmark.py
                cli.py
        tests/
            test_basic.py

Then:

    cd oxta_agentomega
    bash quickstart.sh

Or manually:

    cd oxta_agentomega
    pip install -e .[dev]
    pytest
    agentomega run --iters 50 --samples 5 --agents 3
"""

import os
from pathlib import Path
from textwrap import dedent

ROOT = Path("oxta_agentomega")


def write_file(path: Path, content: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        f.write(dedent(content).lstrip("\n"))


def main():
    if ROOT.exists():
        print(f"[!] Directory {ROOT} already exists. Aborting to avoid overwrite.")
        return

    print(f"[+] Creating OCTA AgentΩ repo at ./{ROOT}")

    # ──────────────────────────────────────────────────────────────────────
    # pyproject.toml
    # ──────────────────────────────────────────────────────────────────────
    write_file(
        ROOT / "pyproject.toml",
        """
        [build-system]
        requires = ["setuptools>=61.0"]
        build-backend = "setuptools.build_meta"

        [project]
        name = "oxta-agentomega"
        version = "0.2.0"
        description = "OCTA AgentΩ — Orthospace Attractor Curriculum Engine with Multi-Agent + Perfect-T Crypt"
        authors = [
            { name = "MineSpace / OCTA", email = "noreply@minespace.us" },
        ]
        readme = "README.md"
        requires-python = ">=3.9"
        dependencies = []

        [project.optional-dependencies]
        dev = [
            "pytest",
        ]

        [project.scripts]
        agentomega = "agentomega.cli:main"

        [tool.setuptools]
        package-dir = {"" = "agentomega_src"}

        [tool.setuptools.packages.find]
        where = ["agentomega_src"]
        """
    )

    # ──────────────────────────────────────────────────────────────────────
    # README.md
    # ──────────────────────────────────────────────────────────────────────
    write_file(
        ROOT / "README.md",
        """
        # OCTA AgentΩ v2 — Orthospace Attractor + Multi-Agent + Perfect-T Crypt

        Generated by `agentomega_bootstrap_v2.py`.

        ## Features

        - **Orthospace**: episodes embedded into a structured feature space:
          `[difficulty, uncertainty, tool_calls, exec_reward]` in [0,1]^4.
        - **Perfect Attractor**: soft attractor in Orthospace defining the
          "alive / frontier" regime.
        - **Curriculum Agent**: generates tasks and adjusts difficulty based on
          attractor-shaped rewards.
        - **Executor Agent**: solves tasks via LLM + PythonTool.
        - **Multi-Agent Swarm**: multiple ExecutorAgents form a swarm; answers
          and tool calls are aggregated.
        - **Perfect-T Crypt layer**: each episode gets a `tcrypt_tag` using
          BLAKE2s + a tiny chaos map, for log integrity / regime fingerprinting.
        - **Self-evolving loop**: difficulty and distributions adapt to keep
          the system near the attractor.
        - **Benchmarks & Metrics**:
          - `agentomega benchmark` runs multiple loops and summarizes performance.
          - `agentomega metrics` summarizes a log file.
        - **Tests**: basic pytest to verify core behavior.

        ## Quickstart

        ```bash
        # after running the bootstrap at project root
        cd oxta_agentomega
        bash quickstart.sh
        ```

        That will:

        - Create a venv
        - `pip install -e .[dev]`
        - Run pytest
        - Run a small benchmark with metrics

        You can then run:

        ```bash
        agentomega run --iters 50 --samples 5 --agents 3
        agentomega benchmark --iters 50 --samples 5 --runs 3 --agents 3
        agentomega metrics --log runs/episodes.jsonl
        ```

        Everything uses a `DummyLLM` that actually solves simple arithmetic via
        `PythonTool`, so the loop is real and not just random noise. You can
        later plug in a real LLM by editing `agentomega/llm.py`.
        """
    )

    # ──────────────────────────────────────────────────────────────────────
    # .gitignore
    # ──────────────────────────────────────────────────────────────────────
    write_file(
        ROOT / ".gitignore",
        """
        __pycache__/
        *.pyc
        .venv/
        .idea/
        .vscode/
        runs/
        *.log
        dist/
        build/
        *.egg-info/
        """
    )

    # ──────────────────────────────────────────────────────────────────────
    # quickstart.sh (automated install + test + benchmark)
    # ──────────────────────────────────────────────────────────────────────
    write_file(
        ROOT / "quickstart.sh",
        """
        #!/usr/bin/env bash
        set -e

        echo "[+] Creating virtual environment .venv"
        python -m venv .venv
        source .venv/bin/activate

        echo "[+] Installing in editable mode with dev extras"
        pip install --upgrade pip
        pip install -e .[dev]

        echo "[+] Running tests"
        pytest

        echo "[+] Running benchmark"
        agentomega benchmark --iters 30 --samples 5 --runs 2 --agents 3

        echo "[+] Done. You can now run:"
        echo "    source .venv/bin/activate"
        echo "    agentomega run --iters 50 --samples 5 --agents 3"
        """
    )
    # make it executable-ish (best effort)
    try:
        os.chmod(ROOT / "quickstart.sh", 0o755)
    except Exception:
        pass

    # ──────────────────────────────────────────────────────────────────────
    # Package layout
    # ──────────────────────────────────────────────────────────────────────
    pkg_root = ROOT / "agentomega_src" / "agentomega"
    pkg_root.mkdir(parents=True, exist_ok=True)

    # __init__.py
    write_file(
        pkg_root / "__init__.py",
        """
        \"\"\"OCTA AgentΩ core package.\"\"\"
        """
    )

    # llm.py
    write_file(
        pkg_root / "llm.py",
        """
        from __future__ import annotations
        import random
        from typing import Protocol

        from .tools import PythonTool


        class BaseLLM(Protocol):
            def generate(self, prompt: str) -> str:
                ...


        class DummyLLM:
            \"\"\"Toy LLM that can solve simple arithmetic via PythonTool.

            This is enough to let the AgentΩ loop actually exhibit structure:
            - Easy tasks become trivial → low uncertainty, low attractor score
            - Harder tasks partially fail → higher uncertainty, interesting regimes
            \"\"\"

            def __init__(self, tool: PythonTool):
                self.tool = tool

            def generate(self, prompt: str) -> str:
                p = prompt.lower()
                # Try to detect an arithmetic expression
                for op in ["+", "-", "*", "/"]:
                    if op in p:
                        try:
                            expr = p.replace("?", " ").split("compute")[-1]
                            expr = expr.split("what is")[-1]
                            expr = expr.strip()
                            allowed = set("0123456789 .+-*/()")
                            expr = "".join(ch for ch in expr if ch in allowed)
                            code = f"_result = {expr}"
                            out = self.tool.run(code)
                            return f"The answer is {out}"
                        except Exception:
                            pass

                # fallback: random-ish numeric answer for diversity
                n = random.randint(1, 1000)
                return f"I think the answer is approximately {n}."


        class OpenAILLM:
            \"\"\"Optional: real LLM wrapper.

            To use:
                - install `openai` (pip install openai)
                - set OPENAI_API_KEY in your environment
                - swap DummyLLM for OpenAILLM in cli.py

            Note: This is a simple wrapper, not production crypto- or safety-grade.
            \"\"\"

            def __init__(self, model: str = "gpt-4.1-mini"):
                try:
                    import openai  # type: ignore
                except ImportError as e:
                    raise RuntimeError(
                        "openai package not installed. "
                        "Install with `pip install openai`."
                    ) from e
                self._openai = openai
                self.model = model

            def generate(self, prompt: str) -> str:
                resp = self._openai.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=256,
                    temperature=0.4,
                )
                return resp.choices[0].message.content or ""
        """
    )

    # tools.py
    write_file(
        pkg_root / "tools.py",
        """
        import math
        from typing import Any, Dict


        class PythonTool:
            \"\"\"Minimal Python interpreter tool with a constrained builtin set.

            This is a toy execution sandbox. Do NOT treat this as a secure
            sandbox for untrusted code in real systems.
            \"\"\"

            def __init__(self, max_lines: int = 20):
                self.max_lines = max_lines

            def run(self, code: str) -> str:
                lines = code.strip().splitlines()
                if len(lines) > self.max_lines:
                    return "ERROR: too many lines for PythonTool"

                safe_globals = {
                    "__builtins__": {
                        "abs": abs,
                        "min": min,
                        "max": max,
                        "sum": sum,
                        "len": len,
                        "range": range,
                        "print": print,
                        "math": math,
                    }
                }
                safe_locals: Dict[str, Any] = {}

                try:
                    exec(code, safe_globals, safe_locals)
                except Exception as e:
                    return f"ERROR: {type(e).__name__}: {e!s}"

                if "_result" in safe_locals:
                    return repr(safe_locals["_result"])
                return repr(safe_locals)
        """
    )

    # tasks.py
    write_file(
        pkg_root / "tasks.py",
        """
        from __future__ import annotations
        import dataclasses
        import random
        from typing import Any, Dict


        @dataclasses.dataclass
        class Task:
            id: str
            difficulty: int
            domain: str
            prompt: str
            metadata: Dict[str, Any]


        class TaskFactory:
            \"\"\"Generates synthetic math / logic tasks by difficulty.\"\"\"

            def __init__(self):
                self._next_id = 0

            def _next_task_id(self) -> str:
                self._next_id += 1
                return f"task_{self._next_id:05d}"

            def math_task(self, difficulty: int) -> Task:
                max_val = 10 * difficulty
                a = random.randint(1, max_val)
                b = random.randint(1, max_val)
                op = random.choice(["+", "-", "*"])
                prompt = f"Compute {a} {op} {b}. Please show your reasoning."
                meta = {
                    "a": a,
                    "b": b,
                    "op": op,
                    "true_answer": eval(f"{a}{op}{b}"),
                }
                return Task(
                    id=self._next_task_id(),
                    difficulty=difficulty,
                    domain="math",
                    prompt=prompt,
                    metadata=meta,
                )

            def logic_task(self, difficulty: int) -> Task:
                # Simple parity / divisibility style reasoning
                n = random.randint(1, 10 * difficulty + 10)
                prompt = (
                    f"Consider the integer {n}. "
                    "Is it divisible by 2, 3, or 5? "
                    "Answer with a short explanation."
                )
                meta = {
                    "n": n,
                    "div2": (n % 2 == 0),
                    "div3": (n % 3 == 0),
                    "div5": (n % 5 == 0),
                }
                return Task(
                    id=self._next_task_id(),
                    difficulty=difficulty,
                    domain="logic",
                    prompt=prompt,
                    metadata=meta,
                )

            def sample_task(self, difficulty: int) -> Task:
                # For now: mostly math, occasional logic
                if random.random() < 0.8:
                    return self.math_task(difficulty)
                return self.logic_task(difficulty)
        """
    )

    # orthospace.py
    write_file(
        pkg_root / "orthospace.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass
        from typing import List
        import math


        @dataclass
        class OrthoPoint:
            \"\"\"A point in Orthospace: a fixed-length feature vector.\"\"\"

            coords: List[float]

            def distance_to(self, other: "OrthoPoint") -> float:
                return math.sqrt(
                    sum((a - b) ** 2 for a, b in zip(self.coords, other.coords))
                )

            def lerp(self, other: "OrthoPoint", t: float) -> "OrthoPoint":
                return OrthoPoint(
                    coords=[a + t * (b - a) for a, b in zip(self.coords, other.coords)]
                )


        class Orthospace:
            \"\"\"Maps episodes into a structured feature space.

            We encode:
                [difficulty, uncertainty, tool_calls, exec_reward]
            normalized into [0, 1] ranges for the attractor.
            \"\"\"

            def __init__(
                self,
                max_difficulty: int = 10,
                max_tool_calls: int = 10,
            ):
                self.max_difficulty = max_difficulty
                self.max_tool_calls = max_tool_calls

            def embed_episode(
                self,
                difficulty: int,
                uncertainty: float,
                tool_calls: int,
                exec_reward: float,
            ) -> OrthoPoint:
                d = difficulty / max(self.max_difficulty, 1)
                u = max(0.0, min(1.0, uncertainty))
                t = min(tool_calls, self.max_tool_calls) / max(self.max_tool_calls, 1)
                r = max(0.0, min(1.0, exec_reward))
                return OrthoPoint(coords=[d, u, t, r])
        """
    )

    # attractor.py
    write_file(
        pkg_root / "attractor.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass
        import math
        from .orthospace import OrthoPoint


        @dataclass
        class PerfectAttractor:
            \"\"\"A soft attractor in Orthospace.

            "Perfect" here is conceptual, not mathematical perfection.
            We define a "sweet spot" in terms of:
                difficulty ~ medium-high
                uncertainty ~ 0.5 (frontier, not trivial or impossible)
                tool_calls ~ moderate-high
                exec_reward ~ moderate-high
            \"\"\"

            center: OrthoPoint
            radius: float

            def score(self, point: OrthoPoint) -> float:
                d = point.distance_to(self.center)
                if d >= self.radius:
                    return 0.0
                # Smooth falloff: cosine bump
                x = d / self.radius
                return 0.5 * (1.0 + math.cos(math.pi * x))

            @classmethod
            def default(cls) -> "PerfectAttractor":
                # [difficulty, uncertainty, tool_calls, exec_reward]
                center = OrthoPoint(coords=[0.7, 0.5, 0.7, 0.7])
                radius = 0.5
                return cls(center=center, radius=radius)
        """
    )

    # rewards.py
    write_file(
        pkg_root / "rewards.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass
        from typing import List

        from .orthospace import Orthospace
        from .attractor import PerfectAttractor


        def compute_uncertainty(answers: List[str]) -> float:
            if not answers:
                return 1.0
            norm = [a.strip().lower() for a in answers]
            counts = {}
            for a in norm:
                counts[a] = counts.get(a, 0) + 1
            majority = max(counts.values())
            frac = majority / len(norm)
            return 1.0 - frac


        def curriculum_reward(
            uncertainty: float,
            tool_calls: int,
            tool_calls_cap: int = 5,
        ) -> float:
            # Parabolic uncertainty reward peaked at 0.5
            u = max(0.0, min(1.0, uncertainty))
            r_u = 1.0 - 4.0 * (u - 0.5) ** 2
            r_u = max(0.0, r_u)

            # Tool usage linear bonus
            t = min(tool_calls, tool_calls_cap) / max(tool_calls_cap, 1)
            r_t = t

            return 0.7 * r_u + 0.3 * r_t


        def executor_reward(answers: List[str], true_answer: str) -> float:
            true_str = str(true_answer).strip().lower()
            norm = [a.strip().lower() for a in answers]
            correct = sum(1 for a in norm if true_str in a)
            return correct / max(len(norm), 1)


        @dataclass
        class RewardShaper:
            orthospace: Orthospace
            attractor: PerfectAttractor

            def full_rewards(
                self,
                difficulty: int,
                answers: List[str],
                tool_calls: int,
                true_answer: str,
            ):
                u = compute_uncertainty(answers)
                r_exec = executor_reward(answers, true_answer)
                r_curr = curriculum_reward(u, tool_calls)

                # Orthospace → attractor
                point = self.orthospace.embed_episode(
                    difficulty=difficulty,
                    uncertainty=u,
                    tool_calls=tool_calls,
                    exec_reward=r_exec,
                )
                r_attr = self.attractor.score(point)

                # Blend: we can tune these weights later
                blended_curr = 0.7 * r_curr + 0.3 * r_attr
                blended_exec = 0.8 * r_exec + 0.2 * r_attr

                return {
                    "uncertainty": u,
                    "executor_raw": r_exec,
                    "curriculum_raw": r_curr,
                    "attractor": r_attr,
                    "curriculum_total": blended_curr,
                    "executor_total": blended_exec,
                }
        """
    )

    # perfect_t_crypt.py
    write_file(
        pkg_root / "perfect_t_crypt.py",
        """
        from __future__ import annotations
        import hashlib
        import json
        import math
        from typing import Any, Dict


        def _flatten_for_hash(record: Dict[str, Any]) -> str:
            \"\"\"Stable JSON-like canonicalization.

            This is *not* a formally verified canonical JSON, but good enough for
            experimental tagging and integrity checks.
            \"\"\"
            return json.dumps(record, sort_keys=True, separators=(",", ":"))


        def _chaotic_mix(seed_bytes: bytes, steps: int = 32) -> bytes:
            \"\"\"Tiny logistic-map based mixer.

            This is an aesthetic chaos step, NOT cryptographically necessary.
            \"\"\"
            if not seed_bytes:
                seed_bytes = b"0"
            x = int.from_bytes(seed_bytes[:8], "big") / float(2**64)
            for _ in range(steps):
                x = 4.0 * x * (1.0 - x)
                if x <= 0.0 or x >= 1.0:
                    x = abs(math.sin(x)) % 1.0
            val = int(x * (2**64 - 1))
            return val.to_bytes(8, "big")


        def perfect_t_tag(record: Dict[str, Any], secret: str = "") -> str:
            \"\"\"Compute a Perfect-T style tag for an episode record.

            Conceptually:
                - Flatten the record into a canonical string.
                - Hash with BLAKE2s.
                - Push through a small chaotic mixer.
                - Hash again with SHA-256.

            This is *not* a formally analyzed cryptosystem. Treat it as an
            experimental integrity / fingerprint tag, not as bulletproof crypto.
            \"\"\"
            payload = _flatten_for_hash(record) + secret
            h = hashlib.blake2s(payload.encode("utf-8"), digest_size=32)
            base = h.digest()
            mixed = _chaotic_mix(base)
            final = hashlib.sha256(base + mixed).hexdigest()
            return final
        """
    )

    # curriculum.py
    write_file(
        pkg_root / "curriculum.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass, field
        from typing import List

        from .tasks import TaskFactory, Task


        @dataclass
        class EpisodeSummary:
            difficulty: int
            reward_curriculum: float


        @dataclass
        class CurriculumAgent:
            start_difficulty: int = 1
            min_difficulty: int = 1
            max_difficulty: int = 10
            history: List[EpisodeSummary] = field(default_factory=list)

            def __post_init__(self):
                self.current_difficulty = self.start_difficulty
                self.task_factory = TaskFactory()

            def propose_task(self) -> Task:
                return self.task_factory.sample_task(self.current_difficulty)

            def record_episode(self, difficulty: int, reward_curriculum: float):
                self.history.append(
                    EpisodeSummary(
                        difficulty=difficulty,
                        reward_curriculum=reward_curriculum,
                    )
                )

            def update_policy(self, window: int = 10):
                if not self.history:
                    return
                recent = self.history[-window:]
                avg = sum(ep.reward_curriculum for ep in recent) / len(recent)

                # Attractor-shaped heuristic:
                #   We want curriculum_total ~ 0.5–0.7.
                if avg > 0.7 and self.current_difficulty < self.max_difficulty:
                    self.current_difficulty += 1
                elif avg < 0.3 and self.current_difficulty > self.min_difficulty:
                    self.current_difficulty -= 1
        """
    )

    # executor.py
    write_file(
        pkg_root / "executor.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass
        from typing import List, Tuple, Protocol

        from .tools import PythonTool
        from .tasks import Task


        class LLMLike(Protocol):
            def generate(self, prompt: str) -> str:
                ...


        @dataclass
        class ExecutorAgent:
            llm: LLMLike
            tool: PythonTool
            tool_call_prob: float = 0.7

            def solve_task(self, task: Task, n_samples: int = 5) -> Tuple[List[str], int]:
                answers: List[str] = []
                tool_calls = 0

                for _ in range(n_samples):
                    use_tool = (task.domain == "math") and (self.tool_call_prob > 0.0)

                    if use_tool:
                        code = self._heuristic_code_from_prompt(task.prompt)
                        if code is not None:
                            tool_calls += 1
                            out = self.tool.run(code)
                            ans = f"Using python, I computed: {out}"
                        else:
                            ans = self.llm.generate(task.prompt)
                    else:
                        ans = self.llm.generate(task.prompt)

                    answers.append(ans)

                return answers, tool_calls

            def _heuristic_code_from_prompt(self, prompt: str):
                p = prompt.lower()
                for op in ["+", "-", "*", "/"]:
                    if op in p:
                        try:
                            expr = p.replace("?", " ").split("compute")[-1]
                            expr = expr.split("what is")[-1]
                            expr = expr.strip()
                            allowed = set("0123456789 .+-*/()")
                            expr = "".join(ch for ch in expr if ch in allowed)
                            return f"_result = {expr}"
                        except Exception:
                            return None
                return None
        """
    )

    # multi_agent.py
    write_file(
        pkg_root / "multi_agent.py",
        """
        from __future__ import annotations
        from dataclasses import dataclass
        from typing import List, Tuple

        from .executor import ExecutorAgent
        from .tasks import Task


        @dataclass
        class MultiExecutorAgent:
            \"\"\"Swarm of executor agents.

            For each task, every executor produces `n_samples` answers.
            We aggregate all answers and sum tool calls. This gives us a
            multi-agent self-consistency field and richer tool usage.
            \"\"\"

            executors: List[ExecutorAgent]

            def solve_task(self, task: Task, n_samples: int = 5) -> Tuple[List[str], int]:
                all_answers: List[str] = []
                total_tool_calls = 0

                for idx, ex in enumerate(self.executors):
                    answers, tool_calls = ex.solve_task(task, n_samples=n_samples)
                    # tag answers with agent index for visibility
                    tagged = [f"[agent{idx}] {a}" for a in answers]
                    all_answers.extend(tagged)
                    total_tool_calls += tool_calls

                return all_answers, total_tool_calls
        """
    )

    # run_loop.py
    write_file(
        pkg_root / "run_loop.py",
        """
        from __future__ import annotations
        import json
        from dataclasses import dataclass
        from pathlib import Path
        from typing import List

        from .curriculum import CurriculumAgent
        from .orthospace import Orthospace
        from .attractor import PerfectAttractor
        from .rewards import RewardShaper
        from .tasks import Task
        from .perfect_t_crypt import perfect_t_tag


        @dataclass
        class Episode:
            task: Task
            answers: List[str]
            tool_calls: int
            reward_dict: dict


        class SelfEvolvingSystem:
            def __init__(
                self,
                curriculum: CurriculumAgent,
                executor,
                log_dir: str = "runs",
                secret: str = "",
            ):
                self.curriculum = curriculum
                self.executor = executor
                self.log_dir = Path(log_dir)
                self.log_dir.mkdir(parents=True, exist_ok=True)

                orthospace = Orthospace(
                    max_difficulty=self.curriculum.max_difficulty,
                    max_tool_calls=10,
                )
                attractor = PerfectAttractor.default()
                self.reward_shaper = RewardShaper(orthospace=orthospace, attractor=attractor)
                self.episodes: List[Episode] = []
                self.secret = secret

            def run(self, iterations: int = 50, samples_per_task: int = 5):
                for it in range(1, iterations + 1):
                    task = self.curriculum.propose_task()
                    answers, tool_calls = self.executor.solve_task(
                        task, n_samples=samples_per_task
                    )

                    true_answer = task.metadata.get("true_answer", "")
                    r = self.reward_shaper.full_rewards(
                        difficulty=task.difficulty,
                        answers=answers,
                        tool_calls=tool_calls,
                        true_answer=str(true_answer),
                    )

                    ep = Episode(
                        task=task,
                        answers=answers,
                        tool_calls=tool_calls,
                        reward_dict=r,
                    )
                    self.episodes.append(ep)

                    self.curriculum.record_episode(
                        difficulty=task.difficulty,
                        reward_curriculum=r["curriculum_total"],
                    )
                    self.curriculum.update_policy(window=10)

                    print(
                        f"[iter {it:03d}] diff={task.difficulty} "
                        f"unc={r['uncertainty']:.3f} "
                        f"r_curr={r['curriculum_total']:.3f} "
                        f"r_exec={r['executor_total']:.3f} "
                        f"r_attr={r['attractor']:.3f} "
                        f"tools={tool_calls}"
                    )

                self._dump_log()
                self._dump_metrics()

            def _dump_log(self):
                path = self.log_dir / "episodes.jsonl"
                with path.open("w", encoding="utf-8") as f:
                    for ep in self.episodes:
                        rec = {
                            "task_id": ep.task.id,
                            "difficulty": ep.task.difficulty,
                            "domain": ep.task.domain,
                            "prompt": ep.task.prompt,
                            "metadata": ep.task.metadata,
                            "answers": ep.answers,
                            "tool_calls": ep.tool_calls,
                            "rewards": ep.reward_dict,
                        }
                        rec["tcrypt_tag"] = perfect_t_tag(rec, secret=self.secret)
                        f.write(json.dumps(rec) + "\\n")
                print(f"\\n[log] wrote {len(self.episodes)} episodes → {path}")

            def _dump_metrics(self):
                # Lightweight metrics summary
                if not self.episodes:
                    return
                import statistics as _stats

                diffs = [ep.task.difficulty for ep in self.episodes]
                curr = [ep.reward_dict["curriculum_total"] for ep in self.episodes]
                exec_r = [ep.reward_dict["executor_total"] for ep in self.episodes]
                attr = [ep.reward_dict["attractor"] for ep in self.episodes]

                metrics = {
                    "episodes": len(self.episodes),
                    "difficulty_mean": float(_stats.mean(diffs)),
                    "difficulty_min": min(diffs),
                    "difficulty_max": max(diffs),
                    "curriculum_total_mean": float(_stats.mean(curr)),
                    "executor_total_mean": float(_stats.mean(exec_r)),
                    "attractor_mean": float(_stats.mean(attr)),
                }

                path = self.log_dir / "metrics.json"
                with path.open("w", encoding="utf-8") as f:
                    json.dump(metrics, f, indent=2)
                print(f"[metrics] wrote summary → {path}")
        """
    )

    # metrics.py
    write_file(
        pkg_root / "metrics.py",
        """
        from __future__ import annotations
        import json
        from pathlib import Path
        from typing import Dict, Any, List


        def summarize_log(log_path: str) -> Dict[str, Any]:
            path = Path(log_path)
            if not path.exists():
                raise FileNotFoundError(f"Log file not found: {path}")

            diffs: List[int] = []
            curr: List[float] = []
            exec_r: List[float] = []
            attr: List[float] = []
            episodes = 0

            with path.open("r", encoding="utf-8") as f:
                for line in f:
                    if not line.strip():
                        continue
                    rec = json.loads(line)
                    episodes += 1
                    diffs.append(int(rec.get("difficulty", 0)))
                    rw = rec.get("rewards", {})
                    curr.append(float(rw.get("curriculum_total", 0.0)))
                    exec_r.append(float(rw.get("executor_total", 0.0)))
                    attr.append(float(rw.get("attractor", 0.0)))

            if episodes == 0:
                raise ValueError("No episodes found in log.")

            import statistics as _stats

            return {
                "episodes": episodes,
                "difficulty_mean": float(_stats.mean(diffs)),
                "difficulty_min": min(diffs),
                "difficulty_max": max(diffs),
                "curriculum_total_mean": float(_stats.mean(curr)),
                "executor_total_mean": float(_stats.mean(exec_r)),
                "attractor_mean": float(_stats.mean(attr)),
            }
        """
    )

    # benchmark.py
    write_file(
        pkg_root / "benchmark.py",
        """
        from __future__ import annotations
        from typing import Dict, Any, List

        from .tools import PythonTool
        from .llm import DummyLLM
        from .executor import ExecutorAgent
        from .multi_agent import MultiExecutorAgent
        from .curriculum import CurriculumAgent
        from .run_loop import SelfEvolvingSystem


        def run_benchmark(
            iters: int = 50,
            samples: int = 5,
            runs: int = 3,
            agents: int = 3,
        ) -> Dict[str, Any]:
            \"\"\"Run multiple self-evolving loops and aggregate metrics.\"\"\"
            all_metrics: List[Dict[str, Any]] = []

            for r in range(runs):
                print(f"=== Benchmark run {r+1}/{runs} ===")
                tool = PythonTool()
                llm = DummyLLM(tool=tool)

                if agents <= 1:
                    executor = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.7)
                else:
                    ex_list: List[ExecutorAgent] = []
                    for i in range(agents):
                        # vary tool_call_prob a bit per agent
                        tprob = 0.4 + 0.2 * i / max(agents - 1, 1)
                        ex_list.append(
                            ExecutorAgent(llm=llm, tool=tool, tool_call_prob=tprob)
                        )
                    executor = MultiExecutorAgent(executors=ex_list)

                curriculum = CurriculumAgent(
                    start_difficulty=1,
                    min_difficulty=1,
                    max_difficulty=10,
                )
                system = SelfEvolvingSystem(
                    curriculum=curriculum,
                    executor=executor,
                    log_dir=f"runs/benchmark_run_{r+1}",
                    secret="benchmark",
                )
                system.run(iterations=iters, samples_per_task=samples)

                # load metrics
                import json
                from pathlib import Path
                mpath = Path(system.log_dir) / "metrics.json"
                with mpath.open("r", encoding="utf-8") as f:
                    m = json.load(f)
                all_metrics.append(m)

            # aggregate
            import statistics as _stats

            def _avg(key: str) -> float:
                return float(_stats.mean(m[key] for m in all_metrics))

            summary = {
                "runs": runs,
                "iters_per_run": iters,
                "samples_per_task": samples,
                "agents": agents,
                "episodes_per_run_mean": _avg("episodes"),
                "difficulty_mean_over_runs": _avg("difficulty_mean"),
                "curriculum_total_mean_over_runs": _avg("curriculum_total_mean"),
                "executor_total_mean_over_runs": _avg("executor_total_mean"),
                "attractor_mean_over_runs": _avg("attractor_mean"),
            }

            print("\\n=== Benchmark summary ===")
            for k, v in summary.items():
                print(f"{k}: {v}")
            return summary
        """
    )

    # cli.py
    write_file(
        pkg_root / "cli.py",
        """
        import argparse
        import json

        from .tools import PythonTool
        from .llm import DummyLLM  # swap with OpenAILLM if desired
        from .executor import ExecutorAgent
        from .multi_agent import MultiExecutorAgent
        from .curriculum import CurriculumAgent
        from .run_loop import SelfEvolvingSystem
        from .metrics import summarize_log
        from .benchmark import run_benchmark


        def main():
            parser = argparse.ArgumentParser(
                prog="agentomega",
                description="OCTA AgentΩ — Orthospace Attractor Curriculum Engine",
            )
            sub = parser.add_subparsers(dest="command", required=True)

            # run
            run_p = sub.add_parser("run", help="Run a self-evolving training loop")
            run_p.add_argument("--iters", type=int, default=50, help="Iterations")
            run_p.add_argument(
                "--samples",
                type=int,
                default=5,
                help="Samples per task for self-consistency (per agent)",
            )
            run_p.add_argument(
                "--agents",
                type=int,
                default=1,
                help="Number of executor agents in the swarm",
            )

            # metrics
            met_p = sub.add_parser("metrics", help="Summarize a log file")
            met_p.add_argument(
                "--log",
                type=str,
                default="runs/episodes.jsonl",
                help="Path to episodes.jsonl",
            )

            # benchmark
            bmk_p = sub.add_parser("benchmark", help="Run benchmark with multiple runs")
            bmk_p.add_argument("--iters", type=int, default=50, help="Iterations")
            bmk_p.add_argument(
                "--samples",
                type=int,
                default=5,
                help="Samples per task for self-consistency (per agent)",
            )
            bmk_p.add_argument(
                "--runs",
                type=int,
                default=3,
                help="Number of benchmark runs",
            )
            bmk_p.add_argument(
                "--agents",
                type=int,
                default=3,
                help="Number of executor agents in the swarm",
            )

            args = parser.parse_args()

            if args.command == "run":
                _cmd_run(iters=args.iters, samples=args.samples, agents=args.agents)
            elif args.command == "metrics":
                _cmd_metrics(log=args.log)
            elif args.command == "benchmark":
                _cmd_benchmark(
                    iters=args.iters,
                    samples=args.samples,
                    runs=args.runs,
                    agents=args.agents,
                )
            else:
                parser.error(f"Unknown command: {args.command}")


        def _build_executor_and_curriculum(agents: int):
            tool = PythonTool()
            llm = DummyLLM(tool=tool)

            if agents <= 1:
                executor = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.7)
            else:
                ex_list = []
                for i in range(agents):
                    # vary tool_call_prob across agents for diversity
                    tprob = 0.4 + 0.2 * i / max(agents - 1, 1)
                    ex_list.append(
                        ExecutorAgent(llm=llm, tool=tool, tool_call_prob=tprob)
                    )
                executor = MultiExecutorAgent(executors=ex_list)

            curriculum = CurriculumAgent(
                start_difficulty=1,
                min_difficulty=1,
                max_difficulty=10,
            )
            return executor, curriculum

        def _cmd_run(iters: int, samples: int, agents: int):
            executor, curriculum = _build_executor_and_curriculum(agents)
            system = SelfEvolvingSystem(
                curriculum=curriculum,
                executor=executor,
                log_dir="runs",
                secret="oxta",
            )
            system.run(iterations=iters, samples_per_task=samples)

        def _cmd_metrics(log: str):
            m = summarize_log(log)
            print(json.dumps(m, indent=2))

        def _cmd_benchmark(iters: int, samples: int, runs: int, agents: int):
            run_benchmark(iters=iters, samples=samples, runs=runs, agents=agents)
        """
    )

    # ──────────────────────────────────────────────────────────────────────
    # tests
    # ──────────────────────────────────────────────────────────────────────
    tests_root = ROOT / "tests"
    tests_root.mkdir(parents=True, exist_ok=True)

    write_file(
        tests_root / "test_basic.py",
        """
        from agentomega.tasks import TaskFactory
        from agentomega.rewards import RewardShaper, Orthospace, PerfectAttractor
        from agentomega.perfect_t_crypt import perfect_t_tag
        from agentomega.tools import PythonTool
        from agentomega.llm import DummyLLM
        from agentomega.executor import ExecutorAgent
        from agentomega.multi_agent import MultiExecutorAgent


        def test_task_factory_math_metadata():
            tf = TaskFactory()
            t = tf.math_task(difficulty=3)
            assert "true_answer" in t.metadata
            a = t.metadata["a"]
            b = t.metadata["b"]
            op = t.metadata["op"]
            assert t.metadata["true_answer"] == eval(f"{a}{op}{b}")


        def test_reward_shaper_keys():
            ortho = Orthospace(max_difficulty=10, max_tool_calls=10)
            attr = PerfectAttractor.default()
            rs = RewardShaper(orthospace=ortho, attractor=attr)
            r = rs.full_rewards(
                difficulty=5,
                answers=["The answer is 10", "The answer is 10"],
                tool_calls=3,
                true_answer="10",
            )
            for key in [
                "uncertainty",
                "executor_raw",
                "curriculum_raw",
                "attractor",
                "curriculum_total",
                "executor_total",
            ]:
                assert key in r


        def test_perfect_t_tag_hex():
            rec = {"a": 1, "b": 2, "nested": {"x": 3}}
            tag = perfect_t_tag(rec, secret="test")
            assert isinstance(tag, str)
            int(tag, 16)  # should be valid hex


        def test_multi_executor_aggregates():
            tool = PythonTool()
            llm = DummyLLM(tool=tool)
            ex1 = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.7)
            ex2 = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.3)
            multi = MultiExecutorAgent(executors=[ex1, ex2])

            tf = TaskFactory()
            t = tf.math_task(difficulty=2)

            answers, tools = multi.solve_task(t, n_samples=2)
            assert len(answers) >= 4  # 2 samples per agent * 2 agents
            assert tools >= 0
        """
    )

    print("[+] Done.")
    print("Next steps:")
    print("  cd oxta_agentomega")
    print("  bash quickstart.sh")


if __name__ == "__main__":
    main()
