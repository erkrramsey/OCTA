{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCTA / T-Selection Experiments\n",
    "\n",
    "This notebook implements the toy experiments described in the OCTA companion paper:\n",
    "\n",
    "1. **Single-agent multi-view prediction** — testing world residue and self-consistency residue.\n",
    "2. **Multi-agent referential game** — testing world residue and consensus residue.\n",
    "\n",
    "Baseline vs OCTA models are compared in both setups.\n",
    "\n",
    "All code is written to run on **CPU or GPU** automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single-Agent Multi-View Experiment\n",
    "\n",
    "This experiment tests whether **self-consistency regularization improves robustness to distribution shift**.\n",
    "\n",
    "- Latent $z$ sampled from Gaussian mixture\n",
    "- Two linear views with noise: $x_1$, $x_2$\n",
    "- Train model to predict $x_2$ from $x_1$\n",
    "\n",
    "Compare:\n",
    "\n",
    "**Baseline** — single-head predictor\n",
    "\n",
    "**OCTA** — shared backbone, multiple heads, KL between head predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiViewToyDataset(Dataset):\n",
    "    def __init__(self, n_samples=10000, d_latent=4, d_view=8, n_components=3, noise_std=0.1, shift=False, seed=0):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.d_latent = d_latent\n",
    "        self.d_view = d_view\n",
    "        self.n_components = n_components\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        if not shift:\n",
    "            self.means = torch.randn(n_components, d_latent)\n",
    "        else:\n",
    "            self.means = torch.randn(n_components, d_latent) + 5.0\n",
    "\n",
    "        self.W1 = torch.randn(d_view, d_latent)\n",
    "        self.W2 = torch.randn(d_view, d_latent)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def sample_latent(self):\n",
    "        k = random.randint(0, self.n_components - 1)\n",
    "        return self.means[k] + 0.5 * torch.randn(self.d_latent)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = self.sample_latent()\n",
    "        x1 = self.W1 @ z + self.noise_std * torch.randn(self.d_view)\n",
    "        x2 = self.W2 @ z + self.noise_std * torch.randn(self.d_view)\n",
    "        return x1.float(), x2.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerBackbone(nn.Module):\n",
    "    def __init__(self, d_in, d_model=64, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(d_in, d_model)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x).unsqueeze(1)\n",
    "        return self.encoder(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, d_view, d_model=64):\n",
    "        super().__init__()\n",
    "        self.backbone = SimpleTransformerBackbone(d_view, d_model)\n",
    "        self.head = nn.Linear(d_model, d_view)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "class OCTAModel(nn.Module):\n",
    "    def __init__(self, d_view, d_model=64, n_heads=3):\n",
    "        super().__init__()\n",
    "        self.backbone = SimpleTransformerBackbone(d_view, d_model)\n",
    "        self.heads = nn.ModuleList([nn.Linear(d_model, d_view) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        return [head(h) for head in self.heads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_world(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "\n",
    "def gaussian_kl(mu1, mu2):\n",
    "    return 0.5 * torch.mean((mu1 - mu2)**2)\n",
    "\n",
    "\n",
    "def compute_octa_loss(preds, target, lambda_world=1.0, lambda_self=0.1):\n",
    "    Dw = sum(mse_world(p, target) for p in preds)\n",
    "    Ds = 0.0\n",
    "    for i in range(len(preds)):\n",
    "        for j in range(i+1, len(preds)):\n",
    "            Ds += gaussian_kl(preds[i], preds[j])\n",
    "\n",
    "    total = lambda_world*Dw + lambda_self*Ds\n",
    "    return total, Dw.item(), Ds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SingleAgentConfig:\n",
    "    batch_size:int = 128\n",
    "    epochs:int = 10\n",
    "    d_view:int = 8\n",
    "    lr:float = 1e-3\n",
    "    lambda_self:float = 0.1\n",
    "    n_heads:int = 3\n",
    "    device:str = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_agent(cfg=SingleAgentConfig()):\n",
    "\n",
    "    train = MultiViewToyDataset(shift=False, seed=0)\n",
    "    test_in = MultiViewToyDataset(shift=False, seed=1)\n",
    "    test_shift = MultiViewToyDataset(shift=True, seed=2)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=cfg.batch_size, shuffle=True)\n",
    "    test_in_loader = DataLoader(test_in, batch_size=cfg.batch_size)\n",
    "    test_shift_loader = DataLoader(test_shift, batch_size=cfg.batch_size)\n",
    "\n",
    "    # Baseline\n",
    "    baseline = BaselineModel(cfg.d_view).to(cfg.device)\n",
    "    optb = torch.optim.Adam(baseline.parameters(), lr=cfg.lr)\n",
    "\n",
    "    baseline_hist = {\"train\":[], \"test_in\":[], \"test_shift\":[]}\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        baseline.train()\n",
    "        total = 0\n",
    "        for x1, x2 in train_loader:\n",
    "            x1 = x1.to(cfg.device)\n",
    "            x2 = x2.to(cfg.device)\n",
    "            pred = baseline(x1)\n",
    "            loss = mse_world(pred, x2)\n",
    "            optb.zero_grad(); loss.backward(); optb.step()\n",
    "            total += loss.item()*x1.size(0)\n",
    "\n",
    "        train_mse = total/len(train)\n",
    "\n",
    "        def eval_model(model, loader):\n",
    "            model.eval(); tot=0; n=0\n",
    "            with torch.no_grad():\n",
    "                for x1,x2 in loader:\n",
    "                    x1=x1.to(cfg.device);x2=x2.to(cfg.device)\n",
    "                    pred=model(x1)\n",
    "                    tot+=F.mse_loss(pred,x2,reduction='sum').item();n+=x1.size(0)\n",
    "            return tot/n\n",
    "\n",
    "        ti = eval_model(baseline,test_in_loader)\n",
    "        ts = eval_model(baseline,test_shift_loader)\n",
    "\n",
    "        baseline_hist[\"train\"].append(train_mse)\n",
    "        baseline_hist[\"test_in\"].append(ti)\n",
    "        baseline_hist[\"test_shift\"].append(ts)\n",
    "\n",
    "        print(f\"[Baseline] Epoch {ep+1} train={train_mse:.4f} in={ti:.4f} shift={ts:.4f}\")\n",
    "\n",
    "    # OCTA\n",
    "    octa = OCTAModel(cfg.d_view, n_heads=cfg.n_heads).to(cfg.device)\n",
    "    opto = torch.optim.Adam(octa.parameters(), lr=cfg.lr)\n",
    "\n",
    "    octa_hist = {\"L\":[],\"Dw\":[],\"Ds\":[],\"test_in\":[],\"test_shift\":[]}\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        octa.train(); Lsum=Dwsum=Dssum=0\n",
    "        for x1,x2 in train_loader:\n",
    "            x1=x1.to(cfg.device);x2=x2.to(cfg.device)\n",
    "            preds=octa(x1)\n",
    "            L,Dw,Ds=compute_octa_loss(preds,x2,lambda_self=cfg.lambda_self)\n",
    "            opto.zero_grad();L.backward();opto.step()\n",
    "            B=x1.size(0)\n",
    "            Lsum+=L.item()*B;Dwsum+=Dw*B;Dssum+=Ds*B\n",
    "\n",
    "        N=len(train)\n",
    "        Lavg=Lsum/N;Dwavg=Dwsum/N;Dsavg=Dssum/N\n",
    "\n",
    "        def eval_octa_model(model,loader):\n",
    "            model.eval();tot=0;n=0\n",
    "            with torch.no_grad():\n",
    "                for x1,x2 in loader:\n",
    "                    x1=x1.to(cfg.device);x2=x2.to(cfg.device)\n",
    "                    preds=model(x1)\n",
    "                    avg_pred=torch.stack(preds).mean(0)\n",
    "                    tot+=F.mse_loss(avg_pred,x2,reduction='sum').item();n+=x1.size(0)\n",
    "            return tot/n\n",
    "\n",
    "        ti=eval_octa_model(octa,test_in_loader)\n",
    "        ts=eval_octa_model(octa,test_shift_loader)\n",
    "\n",
    "        octa_hist[\"L\"].append(Lavg)\n",
    "        octa_hist[\"Dw\"].append(Dwavg)\n",
    "        octa_hist[\"Ds\"].append(Dsavg)\n",
    "        octa_hist[\"test_in\"].append(ti)\n",
    "        octa_hist[\"test_shift\"].append(ts)\n",
    "\n",
    "        print(f\"[OCTA] Epoch {ep+1} L={Lavg:.4f} Dw={Dwavg:.4f} Ds={Dsavg:.4f} in={ti:.4f} shift={ts:.4f}\")\n",
    "\n",
    "    return baseline_hist,octa_hist\n",
    "\n",
    "\n",
    "baseline_hist,octa_hist = run_single_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(baseline_hist['test_in'])+1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, baseline_hist[\"test_in\"], label=\"Baseline in-dist\")\n",
    "plt.plot(epochs, baseline_hist[\"test_shift\"], label=\"Baseline shifted\")\n",
    "plt.plot(epochs, octa_hist[\"test_in\"], label=\"OCTA in-dist\")\n",
    "plt.plot(epochs, octa_hist[\"test_shift\"], label=\"OCTA shifted\")\n",
    "plt.legend();plt.xlabel(\"Epoch\");plt.ylabel(\"MSE\");\n",
    "plt.title(\"Single-Agent Test MSE\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, octa_hist[\"Dw\"], label=\"Δ_world\")\n",
    "plt.plot(epochs, octa_hist[\"Ds\"], label=\"Δ_self\")\n",
    "plt.legend();plt.xlabel(\"Epoch\");plt.ylabel(\"Residue\");\n",
    "plt.title(\"OCTA Residues\")\n",
    "plt.tight_layout();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Agent Referential Game\n",
    "\n",
    "Here we test **consensus residue**.\n",
    "\n",
    "- Speaker sees $x_1$\n",
    "- Sends discrete message token\n",
    "- Listener reconstructs $x_2$\n",
    "- Both form internal beliefs\n",
    "\n",
    "### Baseline\n",
    "- Train only reconstruction loss\n",
    "\n",
    "### OCTA\n",
    "- Train reconstruction + consensus KL between beliefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDataset(Dataset):\n",
    "    def __init__(self, n=20000, d_latent=4, d_view=8, noise=0.1, comps=4, shift=False, seed=0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.n = n\n",
    "        if not shift:\n",
    "            self.means = torch.randn(comps, d_latent)\n",
    "        else:\n",
    "            self.means = torch.randn(comps, d_latent) + 5.0\n",
    "        self.W1 = torch.randn(d_view, d_latent)\n",
    "        self.W2 = torch.randn(d_view, d_latent)\n",
    "        self.noise=noise\n",
    "        self.d_view=d_view\n",
    "        self.d_latent=d_latent\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def sample_z(self):\n",
    "        k = random.randint(0, self.means.size(0)-1)\n",
    "        return self.means[k] + 0.5*torch.randn(self.means.size(1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z=self.sample_z()\n",
    "        x1=self.W1@z+self.noise*torch.randn(self.d_view)\n",
    "        x2=self.W2@z+self.noise*torch.randn(self.d_view)\n",
    "        return x1.float(),x2.float(),z.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    def __init__(self, d_view, d_hidden=64, vocab=16):\n",
    "        super().__init__()\n",
    "        self.vocab=vocab\n",
    "        self.enc=nn.Sequential(nn.Linear(d_view,d_hidden),nn.ReLU(),nn.Linear(d_hidden,d_hidden),nn.ReLU())\n",
    "        self.msg_head=nn.Linear(d_hidden,vocab)\n",
    "        self.belief_head=nn.Linear(d_hidden,d_view)\n",
    "\n",
    "    def forward(self,x1,tau=1.0):\n",
    "        h=self.enc(x1)\n",
    "        probs=F.softmax(self.msg_head(h)/tau,dim=-1)\n",
    "        msg=torch.argmax(probs,dim=-1)\n",
    "        return msg,probs,self.belief_head(h)\n",
    "\n",
    "\n",
    "class Listener(nn.Module):\n",
    "    def __init__(self, d_view, d_hidden=64, vocab=16):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(vocab,d_hidden)\n",
    "        self.dec=nn.Sequential(nn.Linear(d_hidden,d_hidden),nn.ReLU(),nn.Linear(d_hidden,d_view))\n",
    "        self.belief=nn.Linear(d_hidden,d_view)\n",
    "\n",
    "    def forward(self,msg):\n",
    "        e=self.embed(msg)\n",
    "        return self.dec(e),self.belief(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_world(pred,target): return F.mse_loss(pred,target)\n",
    "def kl_cons(mu1,mu2): return 0.5*torch.mean((mu1-mu2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiAgentConfig:\n",
    "    batch:int=128\n",
    "    lr:float=1e-3\n",
    "    epochs:int=15\n",
    "    lambda_cons:float=0.1\n",
    "    vocab:int=16\n",
    "    d_view:int=8\n",
    "    device:str=device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recon(speaker,listener,loader,cfg):\n",
    "    speaker.eval();listener.eval();tot=n=0\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,z in loader:\n",
    "            x1=x1.to(cfg.device);x2=x2.to(cfg.device)\n",
    "            msg,_,_=speaker(x1)\n",
    "            x2_hat,_=listener(msg)\n",
    "            tot+=F.mse_loss(x2_hat,x2,reduction='sum').item();n+=x1.size(0)\n",
    "    return tot/n\n",
    "\n",
    "\n",
    "def train_pair(baseline=False,cfg=MultiAgentConfig()):\n",
    "    train=ConceptDataset(shift=False,seed=0)\n",
    "    test_in=ConceptDataset(shift=False,seed=1)\n",
    "    test_out=ConceptDataset(shift=True,seed=2)\n",
    "\n",
    "    tl=DataLoader(train,batch_size=cfg.batch,shuffle=True)\n",
    "    ti=DataLoader(test_in,batch_size=cfg.batch)\n",
    "    to=DataLoader(test_out,batch_size=cfg.batch)\n",
    "\n",
    "    speaker=Speaker(cfg.d_view,vocab=cfg.vocab).to(cfg.device)\n",
    "    listener=Listener(cfg.d_view,vocab=cfg.vocab).to(cfg.device)\n",
    "\n",
    "    opt=torch.optim.Adam(list(speaker.parameters())+list(listener.parameters()),lr=cfg.lr)\n",
    "\n",
    "    hist={\"world\":[],\"cons\":[],\"test_in\":[],\"test_out\":[]}\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        speaker.train();listener.train();rw=rc=0\n",
    "        for x1,x2,z in tl:\n",
    "            x1=x1.to(cfg.device);x2=x2.to(cfg.device)\n",
    "            msg,_,bs=speaker(x1)\n",
    "            x2_hat,bl=listener(msg)\n",
    "\n",
    "            Dw=mse_world(x2_hat,x2)\n",
    "            Dc=0 if baseline else kl_cons(bs,bl)\n",
    "\n",
    "            loss=Dw+cfg.lambda_cons*Dc\n",
    "            opt.zero_grad();loss.backward();opt.step()\n",
    "\n",
    "            rw+=Dw.item()*x1.size(0);rc+=Dc*x1.size(0)\n",
    "\n",
    "        N=len(train)\n",
    "        rw/=N;rc/=N\n",
    "\n",
    "        ti_m=eval_recon(speaker,listener,ti,cfg)\n",
    "        to_m=eval_recon(speaker,listener,to,cfg)\n",
    "\n",
    "        hist[\"world\"].append(rw)\n",
    "        hist[\"cons\"].append(rc)\n",
    "        hist[\"test_in\"].append(ti_m)\n",
    "        hist[\"test_out\"].append(to_m)\n",
    "\n",
    "        tag=\"Baseline\" if baseline else \"OCTA\"\n",
    "        print(f\"[{tag}] Epoch {ep+1} Δ_world={rw:.4f} Δ_cons={rc:.4f} in={ti_m:.4f} out={to_m:.4f}\")\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "hist_base=train_pair(baseline=True)\n",
    "hist_octa=train_pair(baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1,len(hist_octa['test_in'])+1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs,hist_base['test_in'],label='Baseline in')\n",
    "plt.plot(epochs,hist_base['test_out'],label='Baseline out')\n",
    "plt.plot(epochs,hist_octa['test_in'],label='OCTA in')\n",
    "plt.plot(epochs,hist_octa['test_out'],label='OCTA out')\n",
    "plt.legend();plt.title('Multi-Agent Test MSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs,hist_base['world'],label='Baseline Δ_world')\n",
    "plt.plot(epochs,hist_octa['world'],label='OCTA Δ_world')\n",
    "plt.plot(epochs,hist_octa['cons'],label='OCTA Δ_cons')\n",
    "plt.legend();plt.title('Residues Over Training')\n",
    "\n",
    "plt.tight_layout();plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
