# OCTA
OCTA AGI Labs

# OCTA AgentΩ v2  
Orthospace Attractor + Multi-Agent Swarm + Perfect-T Crypt Tags

Self-evolving curriculum loop for LLM agents.  
You give it an LLM (by default a tiny `DummyLLM`), it:

- Generates math/logic tasks at adaptive difficulty
- Runs a **multi-agent swarm** of executors with a Python tool
- Measures **uncertainty, tool-use, and correctness**
- Embeds each episode into **Orthospace** `[difficulty, uncertainty, tool_calls, exec_reward]`
- Shapes rewards with a **Perfect Attractor** in Orthospace
- Stamps each episode with a **Perfect-T tag** for integrity / regime fingerprinting
- Logs everything and gives you **metrics + benchmarks**

This repo is generated by `agentomega_bootstrap_v2.py` – runtime instructions below assume you’re already inside the generated `oxta_agentomega/` directory.

---

## 1. Requirements

- **Python**: 3.9 or newer
- OS: Linux / macOS / WSL recommended (Windows should work, but paths/venv activation differ slightly)
- Recommended: `git`, `python3`, `pip`

Optional (for real LLMs):

- `openai` Python package
- `OPENAI_API_KEY` in your environment

---

## 2. First-Time Setup

If you haven’t run the bootstrap yet:

```bash
# from the directory where agentomega_bootstrap_v2.py lives
python agentomega_bootstrap_v2.py

cd oxta_agentomega

Now you have the full repo:

oxta_agentomega/
  pyproject.toml
  README.md
  .gitignore
  quickstart.sh
  agentomega_src/
    agentomega/
      __init__.py
      llm.py
      tools.py
      tasks.py
      orthospace.py
      attractor.py
      rewards.py
      perfect_t_crypt.py
      curriculum.py
      executor.py
      multi_agent.py
      run_loop.py
      metrics.py
      benchmark.py
      cli.py
  tests/
    test_basic.py


---

3. Quickstart (one command)

From inside oxta_agentomega/:

bash quickstart.sh

What this does:

1. Creates a local virtualenv: .venv/


2. Activates it


3. Installs the package in editable mode with dev extras:

pip install -e .[dev]


4. Runs the basic test suite:

pytest


5. Runs a small benchmark:

agentomega benchmark --iters 30 --samples 5 --runs 2 --agents 3



When it finishes, you’ll see:

Episode logs under runs/benchmark_run_*/episodes.jsonl

Metrics summaries as runs/benchmark_run_*/metrics.json



---

4. Manual Setup (if you prefer)

Instead of quickstart.sh, you can do it manually:

cd oxta_agentomega

# Create & activate a virtualenv
python -m venv .venv
source .venv/bin/activate   # on Windows: .venv\Scripts\activate

# Install OCTA AgentΩ in editable mode
pip install --upgrade pip
pip install -e .[dev]

# Run tests
pytest


---

5. Runtime Commands

The CLI entrypoint is agentomega (installed by pyproject.toml).

5.1 Run a single self-evolving loop

agentomega run --iters 50 --samples 5 --agents 3

Arguments:

--iters – number of curriculum iterations / episodes (default: 50)

--samples – self-consistency samples per agent (default: 5)

--agents – number of executor agents in the swarm (default: 1)


What happens each iteration:

1. CurriculumAgent proposes a task at the current difficulty.


2. MultiExecutorAgent (swarm) or single ExecutorAgent:

Generates n_samples answers per agent.

Uses a Python tool to evaluate math expressions when helpful.



3. RewardShaper computes:

uncertainty (disagreement across answers)

executor_total (correctness vs ground truth)

curriculum_total (uncertainty + tool use)

attractor (how close the episode is to the Orthospace attractor)



4. Difficulty is adjusted up/down to keep the system near the attractor.


5. The episode is logged with a Perfect-T tag.



Artifacts:

Episodes: runs/episodes.jsonl

Metrics summary: runs/metrics.json



---

5.2 Run a benchmark (multiple runs)

agentomega benchmark --iters 50 --samples 5 --runs 3 --agents 3

Arguments:

--iters – iterations per run

--samples – samples per task per agent

--runs – number of full runs

--agents – number of executor agents in the swarm


Each run creates its own log directory:

runs/
  benchmark_run_1/
    episodes.jsonl
    metrics.json
  benchmark_run_2/
  benchmark_run_3/

At the end, you get a printed aggregated summary across runs (mean difficulty, mean curriculum reward, mean attractor score, etc.).


---

5.3 Summarize any log file

If you have an episodes.jsonl from a run or benchmark, you can summarize it:

agentomega metrics --log runs/episodes.jsonl

or for a specific benchmark run:

agentomega metrics --log runs/benchmark_run_1/episodes.jsonl

This prints JSON like:

{
  "episodes": 50,
  "difficulty_mean": 5.2,
  "difficulty_min": 1,
  "difficulty_max": 10,
  "curriculum_total_mean": 0.61,
  "executor_total_mean": 0.79,
  "attractor_mean": 0.54
}


---

6. What’s Actually Running Under the Hood

6.1 Orthospace

agentomega/orthospace.py

Episodes are encoded as:

[difficulty, uncertainty, tool_calls, exec_reward] ∈ [0,1]^4

difficulty – normalized from [1, max_difficulty]

uncertainty – 0 = perfect agreement, 1 = total disagreement

tool_calls – normalized count of Python tool invocations

exec_reward – fraction of answers that contain the correct result


6.2 Perfect Attractor

agentomega/attractor.py

Defines a soft “sweet spot” region in Orthospace:

Centered around: medium-high difficulty, uncertainty ~0.5, moderate-high tool use, moderate-high exec reward

Returns a smooth score ∈ [0,1] (cosine bump) based on distance to the center.


This attractor score is blended into both:

curriculum_total

executor_total


so that exploration is pulled toward the frontier rather than trivial or impossible regimes.

6.3 Rewards

agentomega/rewards.py

compute_uncertainty(answers) – how much the agents disagree

executor_reward – correctness vs ground truth

curriculum_reward – parabolic reward around uncertainty = 0.5 + tool-use bonus

RewardShaper.full_rewards – combines:

uncertainty

executor_raw / executor_total

curriculum_raw / curriculum_total

attractor




---

6.4 Curriculum Agent

agentomega/curriculum.py

Tracks a history of recent episodes.

Uses rolling average of curriculum_total to adjust difficulty:

If reward too high → increase difficulty

If reward too low → decrease difficulty



This creates an automatic self-tuning curriculum.


---

6.5 Executor + Swarm

agentomega/executor.py / agentomega/multi_agent.py

ExecutorAgent:

Has an LLM + PythonTool

For math tasks, it tries to extract an expression and run it via Python


MultiExecutorAgent:

Wraps a list of ExecutorAgents

Each agent produces n_samples answers

Answers are tagged by [agentX] and aggregated

Tool calls are summed



This gives you multi-agent self-consistency and richer uncertainty / tool-use signals.


---

6.6 Perfect-T Crypt Tag

agentomega/perfect_t_crypt.py

Each episode record is turned into a tag:

1. Flatten the record into a canonical JSON-like string.


2. Hash using BLAKE2s (32-byte digest).


3. Feed the digest through a tiny chaotic logistic map mixer.


4. Hash again with SHA-256.



Result: a 64-char hex string stored as tcrypt_tag on each episode.
This is an experimental integrity / regime fingerprint — not a formally audited cryptosystem.


---

6.7 Logs and Metrics

agentomega/run_loop.py / agentomega/metrics.py

Episodes are written to runs/episodes.jsonl (or runs/benchmark_run_X/episodes.jsonl) with:

Task info, metadata, answers, tool_calls, rewards, tcrypt_tag


A metrics summary is written to runs/metrics.json with basic statistics.


You can build your own analytics by reading the JSONL and filtering by difficulty, uncertainty, attractor score, etc.


---

7. Using as a Python Library

After pip install -e . you can import it in your own code:

from agentomega.tools import PythonTool
from agentomega.llm import DummyLLM
from agentomega.executor import ExecutorAgent
from agentomega.curriculum import CurriculumAgent
from agentomega.run_loop import SelfEvolvingSystem

tool = PythonTool()
llm = DummyLLM(tool=tool)
executor = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.7)
curriculum = CurriculumAgent(start_difficulty=1, min_difficulty=1, max_difficulty=10)

system = SelfEvolvingSystem(
    curriculum=curriculum,
    executor=executor,
    log_dir="runs/custom",
    secret="my_experiment",
)

system.run(iterations=100, samples_per_task=5)


---

8. Swapping in a Real LLM (Optional)

By default, the system uses DummyLLM:

Actually solves many math tasks via PythonTool

Fakes plausible answers for everything else


To plug in a real LLM:

1. Install openai:

pip install openai


2. Set your API key:

export OPENAI_API_KEY="sk-..."


3. In agentomega/llm.py, use OpenAILLM instead of DummyLLM in the builder logic (e.g. in cli.py or your own code):

from agentomega.llm import OpenAILLM
from agentomega.tools import PythonTool
from agentomega.executor import ExecutorAgent

tool = PythonTool()
llm = OpenAILLM(model="gpt-4.1-mini")
executor = ExecutorAgent(llm=llm, tool=tool, tool_call_prob=0.7)


4. Re-run:

agentomega run --iters 50 --samples 5 --agents 3



Important: this is not a production safety / security harness — it’s an experimental attractor engine. Wrap it appropriately before exposing to untrusted inputs.


---

9. Running Tests

To verify everything still works:

cd oxta_agentomega
source .venv/bin/activate     # if not already
pytest

What tests cover:

TaskFactory math metadata correctness

RewardShaper emits all expected keys

perfect_t_tag returns valid hex

MultiExecutorAgent aggregates multiple agents and samples



---

10. GitHub Usage

Typical GitHub flow:

# after running the bootstrap and cd'ing into the repo
git init
git add .
git commit -m "OCTA AgentΩ v2 initial commit"

# add your remote
git remote add origin git@github.com:YOUR_USER/oxta_agentomega.git
git push -u origin main

This README.md is written to be GitHub-ready:

Markdown-only



---

11. TL;DR

From zero to running attractor:

python agentomega_bootstrap_v2.py   # (one time, outside the repo)
cd oxta_agentomega
bash quickstart.sh                  # venv + install + tests + benchmark

# everyday usage
source .venv/bin/activate
agentomega run --iters 50 --samples 5 --agents 3
agentomega metrics --log runs/episodes.jsonl

